{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0dYlNC8/Gd6Sdgd30cVy4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivamVR46/LLM/blob/master/LLM_Setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ipdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL3Van0t4Qu8",
        "outputId": "8701033c-5193-4fa3-c569-06855a40d4a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.10/dist-packages (0.13.13)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from ipdb) (2.0.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.19.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import ipdb # for debugging\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import platform, shutil\n",
        "import requests, zipfile, io\n",
        "\n",
        "#Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#tokenizer\n",
        "import sentencepiece as spm\n",
        "\n",
        "# For better performance\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.allow_tf32 = True\n",
        "\n",
        "#Empty GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MMetpGl1zJ2w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBNHLy9hjvLP",
        "outputId": "06ad2535-39e8-4311-d30d-dd09c5704db7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QP3p5NA_yq7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a2981d-e628-427e-cc83-9901e3f5e7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: You will be using:  cuda\n"
          ]
        }
      ],
      "source": [
        "# ARCHITECTURE PARAMETERS\n",
        "batch_size = 16  # batch size is nothing but data to be divided into batches so that LLM can learn to guess the next sequence of tokens\n",
        "                 # required GPU as it can process these batches parallely; size 8 to 128 is enough for 4GB GPU\n",
        "context = 512    # window (range) that the LLM is going to pay attention to as each input is a token transformed to numbers and tokenizer will\n",
        "                 # have all the possibibilities of token. Paying attention to larger window is good as LLM cab build relationship between tokens but that requires more memory.\n",
        "embed_size = 384 # tokens are transformed to numbers and just by using number LLM cannot know what does it means so tokens are tranformed to embedding,\n",
        "                 # which would be a vector of 384 numbers here where each number is a feature to describe the token.\n",
        "n_layers = 7     #transformer architecture is of number of layers; each layer has 2 things : attention mechanism (communication) and feed forward layer (computation)\n",
        "n_head = 7       # In each attention mechanism, there are number of heads so 384 features of each token splitted into 7 heads and then bring them together then to feed forward layer.\n",
        "BIAS = True\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "\n",
        "lr = 3e-4 # learning rate\n",
        "dropout = 0.05 # prevents overfitting, drop 5% of neurons so that network doesnt memorize everything as it needs to learn to generalize\n",
        "weight_decay = 0.01 # limit the scale of the parameters of the network, so that it doesnt adapt too tightly to the training dataset\n",
        "grad_clip = 1.0 # To prevent gradient overflowing or vanishing issue, we need to provide a range for that\n",
        "\n",
        "# TRAINING PARAMETERS\n",
        "\n",
        "train_iters = 100000 # Number of top level of training iterations\n",
        "eval_interval = 50   # We dont use all training data for training ( to prevent overfitting ), thus take percentage of training data for training and for evaluation the other.\n",
        "eval_iters = 10      # Will make an average over multiple batches rather than just calculate the loss over single batch to get performance data\n",
        "compile = False      # Way to work with Pytorch; where pytorch will make models runs smoother - requires GPUs\n",
        "checkpoint_dir = 'models/'  # Folder where the snapshots of the training process would be stored\n",
        "checkpoint_fn = 'latest.pt' # How to call file that will save the values of the parameters\n",
        "checkpoint_load_fn = 'latest.pt'  # To restart the training from one of the checkpoints so to take the file storing that.\n",
        "dtype = torch.bfloat16     # data type (all the numbers/weights/parameters are numbers with different precision) representation of the numbers, floating 16 bit used\n",
        "\n",
        "# MODE\n",
        "inference = False  # During training, the weights (parameters) would be tweaked until it learns to predict the next token/word from a sequence of words.\n",
        "                   # During inference, provides a new set of input of sequence of words and ask to for an answer. Thus during training this is false else true.\n",
        "\n",
        "# DEVICE\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # Is to know if GPU or not\n",
        "print(\"device: You will be using: \", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOGGING\n",
        "\n",
        "wandb_log = True  # wandb.ai which provides free statictics of the model\n",
        "wandb_project = \"llm1\"\n",
        "wandb_run_name = \"llm1-\"+ datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "if wandb_log:\n",
        "  import wandb\n",
        "  wandb.init(project=wandb_project, name=wandb_run_name)"
      ],
      "metadata": {
        "id": "7aP5ofX_4XVO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "outputId": "85b34974-4ee4-4689-b68f-82fe9fc1cf73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1228, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 295, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 346, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 273, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 252, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-9-47e1d8cc818f>\", line 9, in <cell line: 7>\n",
            "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1243, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 152, in reraise\n",
            "    self.exception(exc)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 43, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 124, in exception\n",
            "    event, hint = sentry_sdk.utils.event_from_exception(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentry_sdk/utils.py\", line 1078, in event_from_exception\n",
            "    \"values\": exceptions_from_error_tuple(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentry_sdk/utils.py\", line 946, in exceptions_from_error_tuple\n",
            "    single_exception_from_error_tuple(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentry_sdk/utils.py\", line 764, in single_exception_from_error_tuple\n",
            "    frames = [\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('wiki.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(text[30000:30300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-N-oMm2jhnI",
        "outputId": "4293f71e-b6c2-4ad5-8c9d-1461c086b0fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "terms.\n",
            "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
            "\n",
            "Vertical Addition\n",
            "\n",
            "The animation above demonstrate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ypZdf6hkz46"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}